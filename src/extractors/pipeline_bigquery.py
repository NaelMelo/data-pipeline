import pandas as pd
import numpy as np
import time
from datetime import datetime
from zoneinfo import ZoneInfo
from tqdm import tqdm
from urllib.error import HTTPError
import re
from google.cloud import bigquery
from google.api_core.exceptions import NotFound, BadRequest
from unidecode import unidecode

from .utils import now_fortaleza

# =========================================================================
# FUN√á√ïES DE LIMPEZA E PREPARA√á√ÉO (Do seu script original, com melhorias)
# =========================================================================


def clean_headers_custom_rules(df: pd.DataFrame, max_length: int = 300) -> pd.DataFrame:
    """
    Limpa os nomes das colunas de um DataFrame aplicando um conjunto de regras customizadas.
    """
    df_clean = df.copy()

    new_cols = df_clean.columns.to_series().map(unidecode)

    new_cols = (
        new_cols.str.replace(r"[()]", "", regex=True)  # Regra: Remover par√™nteses
        .str.replace("/", " ", regex=False)  # Regra: Substituir '/' por espa√ßo
        .str.replace(r"\s+", " ", regex=True)  # Regra: Compactar espa√ßos
        .str.strip()  # Regra: Remover espa√ßos nas bordas
        .str.replace(r"^(\d)", r"col_\1", regex=True)  # Regra: Prefixo para d√≠gitos
        .str.replace(r"_{2,}", "_", regex=True)  # Regra: Compactar underscores
        .str.strip("_")  # Regra: Remover underscores nas bordas
        .str.slice(0, max_length)  # Regra: Limitar comprimento
    )

    # Tratamento de colunas duplicadas
    if new_cols.duplicated().any():
        counts = {}
        final_cols = []
        for col in new_cols:
            if col in counts:
                counts[col] += 1
                final_cols.append(f"{col}_{counts[col]}")
            else:
                counts[col] = 0
                final_cols.append(col)
        df_clean.columns = final_cols
    else:
        df_clean.columns = new_cols

    return df_clean


def converter_tipos_bigquery(df: pd.DataFrame, mapeamento_bq: dict) -> pd.DataFrame:
    """
    Converte os tipos de todas as colunas de um DataFrame.
    - Usa o mapeamento para tipos espec√≠ficos.
    - Converte para STRING qualquer coluna n√£o mapeada.
    """
    print("üîÑ Iniciando convers√£o de tipos...")
    df_copy = df.copy()

    # Itera sobre TODAS as colunas do DataFrame, n√£o apenas as do mapeamento.
    for coluna in df_copy.columns:
        # Pega o tipo do mapeamento. Se n√£o existir, o padr√£o √© 'STRING'.
        tipo_bq = mapeamento_bq.get(coluna, "STRING").upper()

        try:
            if tipo_bq == "DATETIME":
                df_copy[coluna] = pd.to_datetime(df_copy[coluna], errors="coerce", format="mixed", dayfirst=True)
            elif tipo_bq == "DATE":
                df_copy[coluna] = pd.to_datetime(
                    df_copy[coluna], errors="coerce", format="mixed", dayfirst=True
                ).dt.date
            elif tipo_bq == "TIME":
                df_copy[coluna] = pd.to_datetime(df_copy[coluna], errors="coerce", format="mixed").dt.time
            elif tipo_bq == "BOOLEAN":
                mapa_bool = {"true": True, "false": False, "1": True, "0": False, "falso": False, "verdadeiro": True}
                df_copy[coluna] = df_copy[coluna].astype(str).str.lower().str.strip().map(mapa_bool).astype("boolean")
            elif tipo_bq in ["INT64", "INTEGER"]:
                df_copy[coluna] = pd.to_numeric(df_copy[coluna], errors="coerce").astype("Int64")
            elif tipo_bq in ["FLOAT64", "NUMERIC", "FLOAT"]:
                df_copy[coluna] = pd.to_numeric(df_copy[coluna], errors="coerce").astype("Float64")
            else:
                df_copy[coluna] = (
                    df_copy[coluna].astype(str).replace("<NA>", np.nan).replace("nan", np.nan).astype("string")
                )

        except Exception as e:
            print(f"   - ‚ùå Erro ao converter '{coluna}' para {tipo_bq}: {e}")

    print("‚úÖ Convers√£o de tipos finalizada.")
    return df_copy


def gerar_schema_bigquery(df: pd.DataFrame, mapeamento_bq_limpo: dict):
    """
    Gera o schema BigQuery (SchemaField) com base no DataFrame final e no mapeamento.
    """
    print("üß± Gerando schema do BigQuery...")
    schema = []
    for coluna in df.columns:
        # Define STRING como padr√£o se a coluna n√£o estiver no mapeamento
        tipo_bq = mapeamento_bq_limpo.get(coluna, "STRING").upper()
        schema.append(bigquery.SchemaField(coluna, tipo_bq, mode="NULLABLE"))

    print("‚úÖ Schema do BigQuery gerado.")
    return schema


# =========================================================================
# FUN√á√ÉO PRINCIPAL DE ORQUESTRA√á√ÉO E ENVIO
# =========================================================================


def carregar_dados_bigquery(json_ou_df, table_id, mapeamento_bq, valor_periodo=None, delimitador_csv=","):
    """
    Orquestra o processo completo: carrega, limpa, converte e envia dados para o BigQuery.

    Args:
        json_ou_df (str or pd.DataFrame): Caminho para o arquivo JSON/CSV ou um DataFrame.
        table_id (str): ID completo da tabela no BigQuery (ex: 'projeto.dataset.tabela').
        mapeamento_bq (dict): Dicion√°rio com {Nome Original da Coluna: Tipo BigQuery}.
        valor_periodo (str, optional): Valor para a coluna 'Periodo', usado para deletar dados antigos.
        delimitador_csv (str, optional): Delimitador para arquivos CSV. Padr√£o √© ','.
    """
    print(f"üöÄ Iniciando pipeline para a tabela: {table_id}")
    maximo_minutos = 10
    intervalo_segundos = 10
    max_tentativas = (maximo_minutos * 60) // intervalo_segundos
    df = None
    for tentativa in tqdm(range(max_tentativas), desc="‚è≥ Carregando JSON"):
        try:
            if not isinstance(json_ou_df, pd.DataFrame):
                df = pd.read_json(json_ou_df)
                tqdm.write(f"üéØ JSON carregado em {(tentativa * intervalo_segundos)//60} minutos!")
                break
            else:
                df = json_ou_df
                tqdm.write(f"üéØ DataFrame carregados!")
                break
        except HTTPError as e:
            if e.code in (404, 500):
                time.sleep(intervalo_segundos)
            else:
                tqdm.write(f"Erro HTTP inesperado: {e}")
                break
        except Exception as e:
            tqdm.write(f"Ocorreu um erro inesperado: {e}")
            tqdm.write("Interrompendo as tentativas.")
            break

    if df is not None:
        # Encontra as colunas com data e ajusta
        padrao_timestamp = re.compile(r"^\d{4}-\d{2}-\d{2}T\d{2}:\d{2}:\d{2}$")
        colunas_transformadas = []
        for coluna in df.columns:
            if df[coluna].dtype == "object":
                primeiro_valor_valido = df[coluna].dropna().iloc[0] if not df[coluna].dropna().empty else None
                if (
                    primeiro_valor_valido
                    and isinstance(primeiro_valor_valido, str)
                    and padrao_timestamp.match(primeiro_valor_valido)
                ):
                    df[coluna] = df[coluna].str[:10]
                    colunas_transformadas.append(coluna)

        # --- ETAPA 1: Limpeza dos Cabe√ßalhos ---
        print("‚ú® Iniciando limpeza dos cabe√ßalhos...")
        df_limpo = clean_headers_custom_rules(df)
        print("‚úÖ Cabe√ßalhos limpos e padronizados.")

        # --- ETAPA 2: Ajuste do Mapeamento ---
        mapeamento_bq_limpo = {
            clean_headers_custom_rules(pd.DataFrame(columns=[k])).columns[0]: v for k, v in mapeamento_bq.items()
        }

        # --- ETAPA 3: Convers√£o de Tipos ---
        df_convertido = converter_tipos_bigquery(df_limpo, mapeamento_bq_limpo)

        # --- ETAPA 4: Gera√ß√£o do Schema ---
        schema_bq = gerar_schema_bigquery(df_convertido, mapeamento_bq_limpo)

        # --- ETAPA 5: Adicionar Colunas de Metadados ---
        df_convertido["extracao_timestamp"] = now_fortaleza()
        if "extracao_timestamp" not in mapeamento_bq_limpo:
            schema_bq.append(bigquery.SchemaField("extracao_timestamp", "DATETIME"))
        print("‚è≤Ô∏è Adicionada coluna de 'extracao_timestamp'")

        # --- ETAPA 6: L√≥gica de Envio para o BigQuery ---
        client = bigquery.Client()

        if valor_periodo:
            df_convertido["periodo"] = valor_periodo
            if "periodo" not in mapeamento_bq_limpo:
                schema_bq.append(bigquery.SchemaField("periodo", "STRING"))
            print(f"‚ö†Ô∏è Coluna 'periodo' adicionada com valor '{valor_periodo}'.")

            print(f"üîÑ Modo de atualiza√ß√£o: Deletando registros para o per√≠odo '{valor_periodo}'...")
            query = f"DELETE FROM `{table_id}` WHERE periodo = '{valor_periodo}'"
            try:
                query_job = client.query(query)
                query_job.result()
                print(f"‚ö†Ô∏è Registros antigos removidos com sucesso.")
            except NotFound:
                print(f"‚ö†Ô∏è A tabela '{table_id}' n√£o existe ainda. A etapa DELETE foi ignorada.")
            except BadRequest as e:
                if "Unrecognized name: periodo" in str(e):
                    print(
                        f"‚ö†Ô∏è A coluna 'periodo' n√£o existe na tabela. A etapa DELETE foi ignorada (provavelmente √© a primeira carga)."
                    )
                else:
                    raise e

            job_config = bigquery.LoadJobConfig(
                schema=schema_bq,
                write_disposition="WRITE_APPEND",
                create_disposition="CREATE_IF_NEEDED",
            )
        else:
            print("üîÑ Modo de atualiza√ß√£o: Substituindo a tabela inteira (WRITE_TRUNCATE)...")
            job_config = bigquery.LoadJobConfig(
                schema=schema_bq,
                write_disposition="WRITE_TRUNCATE",
                create_disposition="CREATE_IF_NEEDED",
            )

        print(f"üì§ Enviando {len(df_convertido)} linhas para o BigQuery...")
        try:
            job = client.load_table_from_dataframe(df_convertido, table_id, job_config=job_config)
            job.result()
            print(f"‚úÖ Sucesso! Tabela '{table_id}' foi atualizada.")
        except Exception as e:
            print(f"‚ùå Falha ao enviar dados para o BigQuery: {e}")
    else:
        print(f"Falha ao carregar o JSON/DF ap√≥s {maximo_minutos} minutos tentando.")
